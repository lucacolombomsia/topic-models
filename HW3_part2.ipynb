{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    #function to read the corpus into a list of strings\n",
    "    with open(path, 'rb') as file:\n",
    "        data = file.read().decode('utf8', 'surrogateescape')\n",
    "        data = data.splitlines()\n",
    "    return data\n",
    "\n",
    "def reviews_doctors_splitter():\n",
    "    #function to split the doctor names from the text reviews\n",
    "    #start by loading the RateMD corpus in memory\n",
    "    text_list = read_files('ratemd.25k.all.txt')\n",
    "    \n",
    "    #each row in the file is either metadata about the doctor or a review\n",
    "    #we only care about one part of the metadata (doctors' names)\n",
    "    #and one part of the review (the qualitative/text review)\n",
    "    \n",
    "    raw_reviews = []\n",
    "    doctors = []\n",
    "\n",
    "    for x in text_list:\n",
    "        #note that metadata has 4 fields separated by tabs\n",
    "        #whereas reviews have 2 fields separated by tabs\n",
    "        temp = x.split('\\t')\n",
    "        #remove extra spaces\n",
    "        temp = [one_str.strip() for one_str in temp]\n",
    "        if len(temp) == 4:\n",
    "            #out of the metadata, keep only the surname of the doctor\n",
    "            d_name = temp[0].split()\n",
    "            doctors.append(d_name[len(d_name)-1].lower())\n",
    "        elif len(temp) == 2:\n",
    "            #convert reviews to lowercase and tokenize\n",
    "            review = word_tokenize(temp[1].lower())\n",
    "            review = [word.replace('.','') for word in review]\n",
    "            #remove reviews with 3 words or less\n",
    "            if len(review) > 3:       \n",
    "                raw_reviews.append(review)\n",
    "    #make sure there are no duplicates in doctors' names list\n",
    "    doctors = list(set(doctors))\n",
    "    return raw_reviews, doctors\n",
    "\n",
    "def get_unfrequent_words(raw_reviews):\n",
    "    #we want to find the least common words and exclude them from the dictionary\n",
    "    #start by counting how many times each word appears in the corpus\n",
    "    raw_dict = {}\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            try:\n",
    "                raw_dict[word] += 1\n",
    "            except KeyError:\n",
    "                raw_dict[word] = 1\n",
    "    \n",
    "    #get list of words that appeared less than 10 times in the corpus\n",
    "    #this will be later dropped\n",
    "    unfreq_words = [k for k, v in raw_dict.items() if v < 10]\n",
    "    return unfreq_words\n",
    "\n",
    "def get_stop_words():\n",
    "    #function that creates a list of stop words\n",
    "    #start by create list with basic stop words in the English language\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #add some ad-hoc words to the list of stopwords\n",
    "    stop_words.add('dr')\n",
    "    stop_words.add('doctor')\n",
    "    stop_words.add(\"n't\")\n",
    "    stop_words.add(\"'ve\")\n",
    "    stop_words.add(\"'s\")\n",
    "    stop_words.add(\"l\")\n",
    "    stop_words.add('â€™')\n",
    "    stop_words.add('u')\n",
    "    stop_words.add('s')\n",
    "    stop_words.add('ca')\n",
    "    stop_words.add('mo')\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews, doctors = reviews_doctors_splitter()\n",
    "unfreq_words = get_unfrequent_words(raw_reviews)\n",
    "stop_words = get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(single_review, stop_words, doctors, unfreq_words, lemmatize):\n",
    "    #this is the function that performs the cleaning/preprocessing of the corpus\n",
    "    #the function takes 5 arguments:\n",
    "    # - a single review (note: not the entire training corpus, just one review)\n",
    "    # - 3 lists of words we want to be removed from the dictionary\n",
    "    #   these are stop words, doctors' names and infrequent words\n",
    "    # - a logical for whether we want to apply a lemmatizer or not\n",
    "    \n",
    "    #first, filter out stop words, Doctor's names, infrequent words and digits/numbers\n",
    "    #for tokens that pass that first selection, remove punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    clean_review = [regex.sub('', word) for word in single_review if word not in doctors\n",
    "                    and word not in unfreq_words and not word.isdigit()\n",
    "                    and word not in stop_words]\n",
    "    \n",
    "    #remove empty strings\n",
    "    clean_review = list(filter(None, clean_review))\n",
    "    \n",
    "    #apply lemmatizer, when requested by the user\n",
    "    if lemmatize == True:\n",
    "        lemma_verb = [lemmatizer.lemmatize(word,'v') for word in clean_review]\n",
    "        lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "        return lemma_noun\n",
    "    \n",
    "    else:\n",
    "        return clean_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cleaning/preprocessing on each review\n",
    "corpus_clean = [clean(review, stop_words, doctors, unfreq_words, lemmatize = False) for review in raw_reviews]\n",
    "\n",
    "#find a unique id for each unique term {term : id}\n",
    "dictionary = corpora.Dictionary(corpus_clean)\n",
    "dict_size = len(dictionary.token2id)\n",
    "print('The dictionary contains {} terms'.format(dict_size))\n",
    "\n",
    "#convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc_clean) for doc_clean in corpus_clean]\n",
    "\n",
    "#run the LDA model\n",
    "ldamodel = LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20, iterations=2000)\n",
    "\n",
    "#output top 10 words in each topic\n",
    "result = []\n",
    "for i in range(10):\n",
    "    top_words = ldamodel.get_topic_terms(i,10)\n",
    "    result.append([dictionary[x[0]] for x in top_words])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra credit question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the LDA model\n",
    "ldamodel = LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20, iterations=2000)\n",
    "\n",
    "#output top 10 words in each topic\n",
    "result = []\n",
    "for i in range(20):\n",
    "    top_words = ldamodel.get_topic_terms(i,10)\n",
    "    result.append([dictionary[x[0]] for x in top_words])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cleaning/preprocessing on each review\n",
    "#this time, set lemmatize parameter to True\n",
    "corpus_clean = [clean(review, stop_words, doctors, unfreq_words, lemmatize = True) for review in raw_reviews]\n",
    "\n",
    "#find a unique id for each unique term {term : id}\n",
    "dictionary = corpora.Dictionary(corpus_clean)\n",
    "dict_size = len(dictionary.token2id)\n",
    "print('The dictionary contains {} terms'.format(dict_size))\n",
    "\n",
    "#convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc_clean) for doc_clean in corpus_clean]\n",
    "\n",
    "#run the LDA model\n",
    "ldamodel = LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20, iterations=2000)\n",
    "\n",
    "#output top 10 words in each topic\n",
    "result = []\n",
    "for i in range(10):\n",
    "    top_words = ldamodel.get_topic_terms(i,10)\n",
    "    result.append([dictionary[x[0]] for x in top_words])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra credit question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the LDA model\n",
    "ldamodel = LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20, iterations=2000)\n",
    "\n",
    "#output top 10 words in each topic\n",
    "result = []\n",
    "for i in range(20):\n",
    "    top_words = ldamodel.get_topic_terms(i,10)\n",
    "    result.append([dictionary[x[0]] for x in top_words])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
